{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Distributed Data Parallel\n",
    "\n",
    "So far, we have used only a single GPU for training. But you may have multiple GPUs available. How do you use them? That's what we do here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a moment to think about what's going to happen in a multi-GPU setting. In a real world setting, you may have multiple boxes (nodes) and each of it may have multiple GPUs. You have your script for training the model. It's good to imagine it this way: several Python interpreters (depending on the total number of GPUs) are going to run the exact same script in parallel, without being aware of what the others are doing. So if you have a print statement in your script, all your GPUs are going to print it. Each GPU is going to run a *process* in parallel.\n",
    "\n",
    "You generally have a way of identifying each GPU by a unique global ID (rank). You also get the ID of the GPU within that node. PyTorch gives this by setting some environment variables such as RANK and LOCAL_RANK. All GPUs run the exact same script, they only differ by these environment variables.\n",
    "\n",
    "Now, your task as a programmer is to ensure that you setup your script in such a way that you can run the exact same script on multiple GPUs at the same time without running into issues. You need to compute loss properly. You also need to make sure that you are passing different data to each GPU. Also, you have no control over which of the GPUs will start / finish before which other GPU. You have to assume that you cannot predict this.\n",
    "\n",
    "You typically assign the GPU with rank = 0 as the master process. For one time tasks, such as printing the loss, you generally print only if the GPU is the master process.\n",
    "\n",
    "Lastly, you need a way to *destroy* these processes as well for cleaning up the mess you left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, you can do this using distributed data parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import os\n",
    "import torch\n",
    "\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1  # if you have multiple GPUs then, this condition will be true & you want to execute script in parallel\n",
    "\n",
    "if ddp:\n",
    "    init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE']) # the total number of GPUs across all nodes\n",
    "\n",
    "    device = f'cuda:{ddp_local_rank}' # Device name within that node. All nodes index GPUs as cuda:0, cuda:1, etc. Thus, we use local rank\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # if the zeroth GPU, then this will be true. For checkpointint, logging, etc.\n",
    "\n",
    "else:\n",
    "    # non ddp run\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

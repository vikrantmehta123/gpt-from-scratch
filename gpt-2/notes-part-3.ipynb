{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Algorithmic Optimizations\n",
    "\n",
    "We made the model train faster, but now we want to improve it's performance. Here, we will try to use the hyperparameters used in GPT-2 or GPT-3 papers to train our model, and see if we get a better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Betas in Adam: $\\beta _1 = 0.9, \\beta _2=0.95, \\epsilon = 10^{-8} $\n",
    "2. Clipping the global norm of the gradients to 1. Gradient clipping is generally did to handle the problem of exploding gradients. The intuitive reason for this is that if you get a bad batch of data, your loss will be high, and thus gradients will be high also, which you don't want to reflect in the model weights. If the norm of your gradients are above some fixed $c$, then you clip their values at $c$. We do it by: $\\frac{g}{||g||}$. In GPT-3, the global norm was clipped at 1. If norm is increasing / you get a sudden spike, things are bad / unstable (in intiali few iterations, the norm can be very high, which is fine ).\n",
    "\n",
    "I'm not entirely sure about two things, which I need to clarify with someone:\n",
    "- Are we clipping each parameter tensor individually, or concatenating all parameter gradients into one big tensor and then clipping it? Most likely, we're taking the *global* norm- by concatenating all parameter gradients.\n",
    "- When we have models like GPT-2, which has so many parameters, if we're clipping the global norm to 1, wouldn't the weight updates be *really* small because we will be multiplying these scaled gradients again by the learning rate? If they are, why can we use lesser precision- it should cause problems?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... model initialization as before. Here, I am initializing to None because I have to copy a\n",
    "# lot of code, which is useless. So init as previous notebooks.\n",
    "model = None\n",
    "train_loader = None\n",
    "device = 'cpu'\n",
    "\n",
    "# Only optimizer and training loop changes\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-4)\n",
    "\n",
    "for i in range(50):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient clipping\n",
    "\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t1 = time.time() \n",
    "\n",
    "    dt = t1 - t0\n",
    "    tokens_processed = train_loader.B * train_loader.T\n",
    "\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "\n",
    "    print(f\"Step {i:4d} | Loss: {loss.item():.6f} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. GPT-3 uses a cosine decay learning rate scheduler with warm up. They set the maximum learning rate to be 6e-4 in GPT-3 125M. With this learning rate schedule, the lr starts near zero ( not exactly zero ) then increases linearly (linear warmup) till the max learning rate, and then decays in *cosine* form till it reaches the minimum specified learning rate. They set minimum as 10% of the max learning rate. GPT-3 was trained on 300B tokens. At 260B tokens, they arrive at the minimum LR and train with that for the remaining 40B tokens. So they are training with higher learning rate for a lot longer than their \"decayed\" learning rate. \n",
    "\n",
    "Learning rates is an active area of research, and people have come up with a lot of different learning rate schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 50 # maximum optimization \"steps\"\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1 # Min LR is 10% of max lr\n",
    "warmup_steps = 10\n",
    "\n",
    "# PyTorch has schedulers which you can use. But here we implement the same\n",
    "def get_lr(it):\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it + 1) / warmup_steps # (it + 1) to ensure we don't start at zero i.e. when it=0\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    \n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "# ... model initialization as before. Here, I am initializing to None because I have to copy a\n",
    "# lot of code, which is useless. So init as previous notebooks.\n",
    "model = None\n",
    "train_loader = None\n",
    "device = 'cpu'\n",
    "\n",
    "# Only optimizer and training loop changes\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-4)\n",
    "\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    lr = get_lr(step)\n",
    "\n",
    "    # In PyTorch optimizer, there are different param_groups & you iterate over them to set LR.\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient clipping\n",
    "\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t1 = time.time() \n",
    "\n",
    "    dt = t1 - t0\n",
    "    tokens_processed = train_loader.B * train_loader.T \n",
    "\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "\n",
    "    print(f\"Step {i:4d} | Loss: {loss.item():.6f} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec} | lr: {lr:4e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Gradual Batch Size Increase:** In GPT-3, they initially start with smaller batch but then they ramp up linearly the batch size gradually. The intuition for why you'd want to do this is that for the early & easy gains you get by driving some probabilities to zero, you don't need a big batch. That is, all gradients in the early stages are highly correlated- if all gradients are the same, then you don't need a big batch size because you get that information from a smaller batch size also.So you start by small batch, and then for the later training you need bigger batch. But this complicates a bit of arithmetic we do on batches, and it's perhaps doesn't have a major impact on performance, but may increase speed of training. So we have not implemented this.\n",
    "\n",
    "5. **Data Sampling:** GPT-3 sampled data randomly without replacement. We already do this in the DataLoader because it iterates over the dataset and thus, a token once seen is not seen again until next epoch.\n",
    "\n",
    "6. **Weight Decay for Regularization:** GPT-3 has a weight decay of 0.1 for regularizing. You generally want some types of weights to be close to zero. For example, matrix multiplication and embeddings weights. Basically, what you want to do is that the parameters that are 2D or above need to be decayed. But 1D parameters or scalars are not decayed, like biases and layer norms. We weight decay because it forces the optimizer to use *more* weights i.e. distribute the work and doesn't allow any one weight to dominate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... Inside the GPT class\n",
    "import torch\n",
    "\n",
    "\n",
    "def configure_optimizer(self, weight_decay, lr, device):\n",
    "    # Get all params that require gradient i.e. will be updated by optimizer\n",
    "    param_dict = {pn:p for pn, p in self.named_parameters()}\n",
    "    param_dict = {pn:p for pn, p in param_dict.items() if p.requires_grad}\n",
    "\n",
    "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "    non_decay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "    \n",
    "    optim_groups = {\n",
    "        {'params' : decay_params, 'weight_decay': weight_decay}, \n",
    "        {'params': non_decay_params, 'weight_decay': 0.0}\n",
    "    }\n",
    "\n",
    "    # In later versions of PyTorch have this fused, not earlier.\n",
    "    # If this fused parameter is present, then it's again kernel fusion (all params are updated in one kernel) and thus runs faster.\n",
    "    # By default, it is not used.\n",
    "    fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "    use_fused = fused_available and 'cuda' in device\n",
    "\n",
    "    optimizer = torch.optim.AdamW(optim_groups, lr=lr, betas=(0.9, 0.95), eps=1e-8, fused=use_fused )\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "# ... optimizer in the training loop\n",
    "optimizer = model.configure_optimizer(weight_decay=0.1, lr=6e-4, device='cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exist relationships between betas, learning rate, weight decay, and batch size. But the topic is quite deep. Refer to notes from Deep Learning course for some hints. At the moment, we're just copying the hyperparameters from GPT-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Accumulation\n",
    "\n",
    "For GPT-3 125M model, they used a batch size of 0.5 million tokens in one batch. But we can't do that because GPU will get exhausted. But we do need a bigger batch size because it is correlated with learning rate, other hyperparameters and some of our layers. So we need some way of *simulating* the higher batch size. For that we have *Gradient Accumulation*. You would keep accumulating gradients till you reach your desired batch size (in number of tokens), and only then you would do the update using the optimizer.\n",
    "\n",
    "Consider the following simple math:\n",
    "```python\n",
    "max_tokens_in_batch = 524288  # tokens we want to process in one batch 2^19\n",
    "B = 16\n",
    "T = 1024 # B*T = tokens we are going to pass in the loop\n",
    "\n",
    "# (2^19) / (16 * 1024) = 32 = number of times we would run the loop to accumulate the gradients and only then update.\n",
    "# i.e. we would accumulate gradients for 32 'batches', and only then update the weights and reset gradients to simulate desired batch size.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there is one subtle issue here. Cross Entropy Loss is calculated as average over the batch. So the dividing factor is the batch size. With gradient accumulation, you have to be careful about the dividing factor. Because with the micro batch of $B \\times T$, your dividing factor is going to be different. \n",
    "\n",
    "One simple fix would be:\n",
    "After you compute `loss.backward()`, divide the loss again by 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch_size = 524288 # 2^19 tokens in one batch\n",
    "B = 16 \n",
    "T = 1024\n",
    "grad_accum_steps = total_batch_size / (B*T)\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    accumulated_loss = 0.0\n",
    "    # Accumulate gradients for some time before you update\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y) \n",
    "\n",
    "        loss = loss / grad_accum_steps\n",
    "        accumulated_loss += loss.detach() # To keep track of how much loss we accumulated over micro batches for printing\n",
    "        loss.backward() # accumulate gradients\n",
    "\n",
    "    # Rest of the loop stays same\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t1 = time.time() \n",
    "\n",
    "    dt = t1 - t0\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps # You process more tokens in one batch\n",
    "\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "\n",
    "    # Print accumulated loss\n",
    "    print(f\"Step {i:4d} | Loss: {accumulated_loss.item():.6f} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec} | lr: {lr:4e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set $B$ to be as the high as GPU can manage. The higher it is, the faster the optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

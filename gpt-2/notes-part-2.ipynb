{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Optimizing the Model\n",
    "\n",
    "So far, we've initialized the GPT-2 model as described in the paper. Now we would want to optimize the model such that we speed up training and potentially, get better performance. So, we'll start with the code we had by the end of last part and then build on that. \n",
    "\n",
    "Be aware though, that these optimizations are not going to work on the CPU but rather a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import math\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config:GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1 # Just to identify the layer which we want to scale down by 1 / sqrt(N)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size)) \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # Batch Size, Sequence Length, Embedding Dim\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1 # Just to identify the layer which we want to scale down by 1/sqrt(N)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config:GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), \n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), \n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), \n",
    "            ln_final = nn.LayerNorm(config.n_embd)\n",
    "        )) \n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights) # apply func iterates over all submodules, and calls _init_weights on it\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (self.config.n_layer * 2) ** -0.5 # Scale down the weights by 1/sqrt(N) \n",
    "\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=std) # initialize linear layer with zero mean and 0.02 stdev\n",
    "        \n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias) # Initialize the bias to zero if it exists\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=0.02) # Embedding layer also init with zero mean and 0.02 stdev\n",
    "\n",
    "        # We're not initializing the LayerNorm because the PyTorch default is what GPT-2 has also used\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "\n",
    "        assert T <= self.config.block_size\n",
    "\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        x = pos_emb + tok_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_final(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "\n",
    "        print(f\"1 Epoch = {len(self.tokens) // (B * T)} Batches\") # In one epoch, we're going to see these many batches, and then start again\n",
    "\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        buf = self.tokens[self.current_position: self.current_position + (B*T+1)]\n",
    "        x = (buf[:-1]).view(B, T)\n",
    "        y = (buf[1:]).view(B, T)\n",
    "\n",
    "        self.current_position += B * T\n",
    "\n",
    "        # Start again if you reach the end of the dataset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Epoch = 2640 Batches\n",
      "After 50 steps, Loss is: 6.799213886260986\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "train_loader = DataLoaderLite(B=4, T=32)\n",
    "\n",
    "# Iterate for some epochs and optimize\n",
    "for i in range(50):\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(f\"After 50 steps, Loss is: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When optimizing, you should always start with:\n",
    "\n",
    "1. What hardware do you have?\n",
    "2. What does it offer?\n",
    "3. Are you fully utilizing it?\n",
    "\n",
    "In PyTorch, by default, all tensors are of dtype float32. That is, each tensor is occupying 32 bits of memory- including parameters, activations, etc. Empirically, for deep learning, 32 bit float representation is too much. You can lower this precision for deep learning but still get good enough models. This can speed up quite a bit because you have much lower memory to move around. Because memory bandwidth is the bottleneck for GPU workloads. That is, most of the time, the tensor cores just sit idle because you're moving memory around to feed them. If you're getting 60% of hardware utilization, you're doing quite well.\n",
    "\n",
    "For training, you still want floats (i.e. not integers), but precision can be lower. However, during inference time, you can use integer precision and still get decent results.\n",
    "\n",
    "Inspect your hardware and GPU. How many TFLOPS does it offer theoretically? 1 TFLOPS = 1 Trillion Floating Point Operations. If you go down the precision, the FLOPS increases quite a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Cores\n",
    "\n",
    "What are tensor cores? Tensor cores are basically simple instructions for the GPU. These do a $4 \\times 4$ matrix multiplication. That is, when you pass a big matrix multiplication to GPU, it breaks down the matrix multiplication into these $4 \\times 4$ units, and does this small matmul in parallel. And deep learning, is mostly matrix multiplication!\n",
    "\n",
    "For reference, look at the white paper on the GPU architecture that you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating Point Precisions\n",
    "\n",
    "1. FP32 is 32 bits\n",
    "2. TF32 is again 32 bits. Exactly same as FP32 but the mantissa is truncated by 13 bits- exponent stays same. Thus, 19 Bits are used. This is all handled internally in hardware- so nothing has to change in your code, which means in your code the dtype is still float32. Empirically, this is almost the same as FP32.\n",
    "3. FP16: This truncates the exponent as well as the mantissa. This is bad! You can lose a lot of important information during training with this. Range of numbers itself is changed. Historically, this came first and thus, it's there. This required gradient scalers.\n",
    "4. BF16: Exponent is the same. But mantissa is cropped even further. Only 7 mantissa bits. Range of numbers is the same, but the precision is low. This doesn't require any gradient scalers, etc. We lose some precision but it runs faster so you can train for longer to make up for that precision. `autocast` in PyTorch does this. Not all tensors are converted to bfloat16. Normalization, loss calculations, etc. are not converted. PyTorch has some internal rules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU and CPU\n",
    "\n",
    "CPU just orders work on the GPU. That is, it queues up a lot of work on the GPU. So, if you want to wait for the GPU to finish it's execution, you need to use: `torch.cuda.synchronize()`.\n",
    "\n",
    "Also, you don't want to let the space on GPU go free. So keep increasing the batch size by powers of two till you can squeeze your model training on the GPU (i.e. basically, right before you get out of memory errors).\n",
    "\n",
    "When measuring speed, tokens processed per second is a more objective measure than time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing: Caveat\n",
    "\n",
    "1. If you're timing your work, be careful to use `torch.cuda.synchronize()`.\n",
    "2. Don't rely too much on the first time. Because PyTorch internally may be doing a lot of initialization, etc., which may slow the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Epoch = 330 Batches\n",
      "Step 0 Loss: 10.953959465026855 Tok / Sec: 111.19398488516616\n",
      "Step 1 Loss: 9.50391674041748 Tok / Sec: 118.7039687585358\n",
      "Step 2 Loss: 9.123509407043457 Tok / Sec: 117.93089306876011\n",
      "Step 3 Loss: 8.86156177520752 Tok / Sec: 118.89022221651372\n",
      "Step 4 Loss: 8.8906831741333 Tok / Sec: 116.95544750490332\n",
      "Step 5 Loss: 8.379107475280762 Tok / Sec: 120.66237026694736\n",
      "Step 6 Loss: 8.229143142700195 Tok / Sec: 117.54321952043479\n",
      "Step 7 Loss: 8.006213188171387 Tok / Sec: 116.16962268089722\n",
      "Step 8 Loss: 7.927321910858154 Tok / Sec: 119.23626511306877\n",
      "Step 9 Loss: 7.698894500732422 Tok / Sec: 117.02392237857198\n",
      "Step 10 Loss: 7.291344165802002 Tok / Sec: 117.91372372453168\n",
      "Step 11 Loss: 7.626772403717041 Tok / Sec: 120.76976891744894\n",
      "Step 12 Loss: 7.092843532562256 Tok / Sec: 119.29011028572712\n",
      "Step 13 Loss: 7.251816749572754 Tok / Sec: 120.72310333425415\n",
      "Step 14 Loss: 6.97932767868042 Tok / Sec: 121.88466098665633\n",
      "Step 15 Loss: 6.898041725158691 Tok / Sec: 117.43754670194578\n",
      "Step 16 Loss: 6.626178741455078 Tok / Sec: 118.65045136826284\n",
      "Step 17 Loss: 6.556342601776123 Tok / Sec: 118.92140965500789\n",
      "Step 18 Loss: 6.368691921234131 Tok / Sec: 116.10667264857125\n",
      "Step 19 Loss: 6.981910228729248 Tok / Sec: 116.62456599106723\n",
      "Step 20 Loss: 6.130266189575195 Tok / Sec: 119.26515378060503\n",
      "Step 21 Loss: 6.668410301208496 Tok / Sec: 114.47873864419572\n",
      "Step 22 Loss: 6.118858814239502 Tok / Sec: 119.1379499534138\n",
      "Step 23 Loss: 5.942072868347168 Tok / Sec: 120.65512653630682\n",
      "Step 24 Loss: 6.238551616668701 Tok / Sec: 116.34410019722304\n",
      "Step 25 Loss: 6.219540596008301 Tok / Sec: 114.7089386345772\n",
      "Step 26 Loss: 6.278885841369629 Tok / Sec: 115.78500176253343\n",
      "Step 27 Loss: 6.0968170166015625 Tok / Sec: 119.73643331972221\n",
      "Step 28 Loss: 6.814990520477295 Tok / Sec: 118.67031800102072\n",
      "Step 29 Loss: 6.255251884460449 Tok / Sec: 120.93502253955964\n",
      "Step 30 Loss: 6.205927848815918 Tok / Sec: 121.20144296002195\n",
      "Step 31 Loss: 6.164953231811523 Tok / Sec: 122.70929840145851\n",
      "Step 32 Loss: 6.574361324310303 Tok / Sec: 122.18265618355201\n",
      "Step 33 Loss: 6.473834991455078 Tok / Sec: 122.06873076088364\n",
      "Step 34 Loss: 6.393869400024414 Tok / Sec: 121.24467302038758\n",
      "Step 35 Loss: 6.021205425262451 Tok / Sec: 116.9408056653909\n",
      "Step 36 Loss: 5.975288391113281 Tok / Sec: 118.54666958467583\n",
      "Step 37 Loss: 6.363707542419434 Tok / Sec: 120.36563001784171\n",
      "Step 38 Loss: 5.883272647857666 Tok / Sec: 121.3826815297477\n",
      "Step 39 Loss: 6.533301830291748 Tok / Sec: 112.75303492396306\n",
      "Step 40 Loss: 6.457479476928711 Tok / Sec: 118.15345827489575\n",
      "Step 41 Loss: 6.546182155609131 Tok / Sec: 112.90696820846797\n",
      "Step 42 Loss: 6.314466953277588 Tok / Sec: 120.43182881288531\n",
      "Step 43 Loss: 6.420978546142578 Tok / Sec: 118.95959818770112\n",
      "Step 44 Loss: 6.439238548278809 Tok / Sec: 120.61108968301733\n",
      "Step 45 Loss: 6.3773345947265625 Tok / Sec: 119.83210810696193\n",
      "Step 46 Loss: 7.2649149894714355 Tok / Sec: 120.14230596660046\n",
      "Step 47 Loss: 6.981866359710693 Tok / Sec: 119.38946669444937\n",
      "Step 48 Loss: 7.2344279289245605 Tok / Sec: 119.92633858777577\n",
      "Step 49 Loss: 7.058406829833984 Tok / Sec: 120.30595356261274\n"
     ]
    }
   ],
   "source": [
    "# This sets precision to TF32. Include at the top of your script\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "import time\n",
    "\n",
    "train_loader = DataLoaderLite(B=1, T=1024)\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for i in range(50):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1-t0) * 1000 # Time diff in milliseconds\n",
    "\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1- t0)\n",
    "\n",
    "    print(f\"Step {i} Loss: {loss.item()} Tok / Sec: {tokens_per_sec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.compile\n",
    "\n",
    "This compiles your PyTorch code- speeds up the code that is slower due to python overhead and GPU read-write ( finds out optimizations in your code and applies them )! So makes your code go much faster. Compilation may take some time but the running of the model will be much faster.\n",
    "\n",
    "Kernel Fusion:\n",
    "\n",
    "Basically, you want to avoid moving data from HBM to GPU again and again. So you want to look at multiple operations on the same data that you can do without moving the memory again. That is, move it once, do a bunch of operations, and move it back. ( As opposed to move it once then do one operation and move it back, and repeat. )\n",
    "\n",
    "Almost always use `torch.compile` unless debugging.\n",
    "\n",
    "I've also included the bfloat16 code in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "train_loader = DataLoaderLite(B=1, T=1024)\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Only loss calc and forward pass needs to be wrapped in here. Nothing else. Again, will work with GPUs\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1-t0) * 1000 # Time diff in milliseconds\n",
    "\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1- t0)\n",
    "\n",
    "    print(f\"Step {i} Loss: {loss.item()} Tok / Sec: {tokens_per_sec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flash Attention\n",
    "\n",
    "Not all operations are found by `torch.compile`. For example, it does not find flash attention. Flash attention is an exact, full self-attention but is a architecture aware. Fast attention is again a kernel-fusion operation. So faster. To add flash attention, you have to change a bit of code in the `CausalSelfAttention` class. Flash attention fuses $softmax(\\frac{QK^T}{\\sqrt{d}})V$ into one fused operation. It's a different algorithm, and thus `torch.compile()` doesn't find it. Further, flash attention does more FLOPS than self-attention but since it is a kernel fusion operation, it doesn't have a lot of read-writes to the HBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config:GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1 # Just to identify the layer which we want to scale down by 1 / sqrt(N)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size)) \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # Batch Size, Sequence Length, Embedding Dim\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # The attention multiplication: Replace it with F.scaled_dot_product_attention\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # y = att @ v\n",
    "\n",
    "        # Flash attention\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ugly Numbers, Nice Numbers\n",
    "\n",
    "**Nice numbers:** The numbers that have many powers of 2 in them. That is, you can divide those numbers by two a lot of times. 64 is awesome. 24 is also decent.\n",
    "\n",
    "**Ugly Numbers:** The opposite of nice numbers- the numbers that cannot be divided by 2 a lot of times. Prime numbers are the worst, odd numbers are also bad. For example, 13 and 25 are ugly numbers. \n",
    "\n",
    "The reason we're discussing this is that a lot of kernels in CUDA are implemented in terms of powers of two (*block tiles*). But if your input is not a nice number, then CUDA kernels have boundary kernels written for the remaining part and these boundary kernels can be a lot slower.\n",
    "\n",
    "The simple fix for this: Scan your code for nice and ugly numbers. If you find ugly numbers, replace them with the next power of two. Yes you will be increasing the number of parameters in your model, and the number of FLOPs will increase but still your code will run much faster.\n",
    "\n",
    "In our code, `vocab_size` = 50,257, which is a very ugly number. So we increase it to 50,304 which has a lot of powers of two in it. (Note: you can't randomly change the sizes and hope that all works well. You may have to check if it is breaking anything. In this case, it doesn't so we can simply replace vocab_size with the new one). We're almost adding fake tokens and the model will drive their probability to zero but because we made it a nice number, we will do more FLOPs but get faster time due to nice numbers. \n",
    "\n",
    "`torch.compile()` doesn't find this also.\n",
    "\n",
    "Depending PyTorch version, the impact of these numbers can be huge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoaderLite(B=1, T=1024)\n",
    "model = GPT(GPTConfig(vocab_size=50304)) # Overwrite vocab_size with 50304\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "## Rest of the code can stay the same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

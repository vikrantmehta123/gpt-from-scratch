{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Optimizing the Model\n",
    "\n",
    "So far, we've initialized the GPT-2 model as described in the paper. Now we would want to optimize the model such that we speed up training and potentially, get better performance. So, we'll start with the code we had by the end of last part and then build on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import math\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config:GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1 # Just to identify the layer which we want to scale down by 1 / sqrt(N)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size)) \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # Batch Size, Sequence Length, Embedding Dim\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1 # Just to identify the layer which we want to scale down by 1/sqrt(N)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config:GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), \n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), \n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), \n",
    "            ln_final = nn.LayerNorm(config.n_embd)\n",
    "        )) \n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights) # apply func iterates over all submodules, and calls _init_weights on it\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (self.config.n_layer * 2) ** -0.5 # Scale down the weights by 1/sqrt(N) \n",
    "\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=std) # initialize linear layer with zero mean and 0.02 stdev\n",
    "        \n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias) # Initialize the bias to zero if it exists\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=0.02) # Embedding layer also init with zero mean and 0.02 stdev\n",
    "\n",
    "        # We're not initializing the LayerNorm because the PyTorch default is what GPT-2 has also used\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "\n",
    "        assert T <= self.config.block_size\n",
    "\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        x = pos_emb + tok_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_final(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "\n",
    "        print(f\"1 Epoch = {len(self.tokens) // (B * T)} Batches\") # In one epoch, we're going to see these many batches, and then start again\n",
    "\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        buf = self.tokens[self.current_position: self.current_position + (B*T+1)]\n",
    "        x = (buf[:-1]).view(B, T)\n",
    "        y = (buf[1:]).view(B, T)\n",
    "\n",
    "        self.current_position += B * T\n",
    "\n",
    "        # Start again if you reach the end of the dataset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Epoch = 2640 Batches\n",
      "After 50 steps, Loss is: 6.799213886260986\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "train_loader = DataLoaderLite(B=4, T=32)\n",
    "\n",
    "# Iterate for some epochs and optimize\n",
    "for i in range(50):\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(f\"After 50 steps, Loss is: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When optimizing, you should always start with:\n",
    "\n",
    "1. What hardware do you have?\n",
    "2. What does it offer?\n",
    "3. Are you fully utilizing it?\n",
    "\n",
    "In PyTorch, by default, all tensors are of dtype float32. That is, each tensor is occupying 32 bits of memory- including parameters, activations, etc. Empirically, for deep learning, 32 bit float representation is too much. You can lower this precision for deep learning but still get good enough models. This can speed up quite a bit because you have much lower memory to move around. Because memory bandwidth is the bottleneck for GPU workloads. That is, most of the time, the tensor cores just sit idle because you're moving memory around to feed them. If you're getting 60% of hardware utilization, you're doing quite well.\n",
    "\n",
    "For training, you still want floats (i.e. not integers), but precision can be lower. However, during inference time, you can use integer precision and still get decent results.\n",
    "\n",
    "Inspect your hardware and GPU. How many TFLOPS does it offer theoretically? 1 TFLOPS = 1 Trillion Floating Point Operations. If you go down the precision, the FLOPS increases quite a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Cores\n",
    "\n",
    "What are tensor cores? Tensor cores are basically simple instructions for the GPU. These do a $4 \\times 4$ matrix multiplication. That is, when you pass a big matrix multiplication to GPU, it breaks down the matrix multiplication into these $4 \\times 4$ units, and does this small matmul in parallel. And deep learning, is mostly matrix multiplication!\n",
    "\n",
    "For reference, look at the white paper on the GPU architecture that you are using."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCREbwdydiJl"
      },
      "source": [
        "# GPT Tokenizer from Scratch\n",
        "\n",
        "In this notebook, I created a tokenizer from scratch following Andrej Karpathy's tutorial on the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4BgZ_bsddZF",
        "outputId": "ff080c45-bc03-443c-8b4c-9023ba3255ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(533, 616)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
        "\n",
        "tokens = text.encode('utf-8')\n",
        "tokens = list(map(int, tokens))\n",
        "len(text), len(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detour: Unicode and UTF\n",
        "\n",
        "One key thing to keep in mind while thinking about Unicode and UTF-8 is that Unicode just defines a character to integer mapping. It does not define any specific binary representation for the character.\n",
        "\n",
        "UTF-8 on the other hand takes the mapping provided by Unicode and provides a binary representation for it. If all the unicode characters are to be represented using a naive way, then what's going to happen is that for each character you would need about 32 bits, which is extremely wasteful.\n",
        "\n",
        "So what UTF-8 does is that it specifies a variable length encoding. Each character is represented as a *stream* of bytes. But you would also need a way to specify the starting point, number of bytes, etc. for each character that you are encoding.\n",
        "\n",
        "\n",
        "If the character is ASCII ( has integer 0 to 127 ), the UTF-8 representation of it starts as: ```0yyyzzzz```\n",
        "\n",
        "For other characters, what UTF-8 does is that the first byte specifies how many bytes are there in the sequence. This first byte is part of that.\n",
        "\n",
        "For example, consider the following UTF-8 encoding.\n",
        "\n",
        "```\n",
        "1110xxxx 10xxxxxx 10xxxxxx\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Here, the initial byte has 3 ones followed by a zero. That means that for this character, there are 3 bytes in total. `x` is the actual binary representation, and `10` in the subsequent bytes represents the *continuation*.\n",
        "\n",
        "UTF-8 uses 8-bit values in its encoding. There are 16-bit, 32-bit versions also but they are not used due to compatibility issues.\n",
        "\n",
        "UTF-8 uses the following rules:\n",
        "\n",
        "- If the code point is < 128, it's represented by the corresponding byte value. That is, it will be returned as a single byte object, and not as a list / sequence of bytes.\n",
        "\n",
        "- If the code point is >= 128, it's turned into a sequence of two, three, or four bytes, where each byte of the sequence is between 128 and 255. That is, to extract the sequence, you will need lists.\n",
        "\n",
        "Think about what this means, though. Because you have specified this particular format, there are going to be some numbers as per UTF that cannot be converted to any character. For example, take the number `10000000`. This is one byte and it doesn't start with a 0. So UTF-8 doesn't have a way of decoding this number!\n",
        "\n",
        "So what should it do? In Python, the `encode` and `decode` functions take an optional error parameter. If the number cannot be decoded, then we can replace that particular number with a `ï¿½`.\n",
        "\n",
        "I have referred to the following videos and articles to know a bit more about Unicode and UTF-8.\n",
        "\n",
        "- [Computerphile on Unicode & UTF-8](https://youtu.be/MijmeoH9LT4?si=9ZJIAK8xHRQbrEsz)\n",
        "- [UTF-8 Wikipedia](https://en.wikipedia.org/wiki/UTF-8)\n",
        "- [Unicode Python HOWTO](https://docs.python.org/3/howto/unicode.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('ï¿½abc', b'A', [240, 159, 152, 137])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = 'ğŸ˜‰' # Hex Repr in Unicode: 'f09f9889', binary Repr in Unicode: [240, 159, 152, 137]. In hex, two characters represent one byte. # F0 in hex = 240, 9F in hex = 159, etc.\n",
        "\n",
        "# Encode as a stream of bytes since the Unicode representation is >= 128\n",
        "utf_enc = s.encode('utf-8')\n",
        "utf_enc_hex = s.encode('utf-8').hex()\n",
        "utf_enc_list = list(s.encode('utf-8'))\n",
        "\n",
        "# encode a single byte\n",
        "utf_single_byte = 'A'.encode('utf-8')\n",
        "\n",
        "# encode with invalid encoding\n",
        "invalid_enc = b'\\x80abc'.decode('utf-8', 'replace')\n",
        "\n",
        "invalid_enc, utf_single_byte, utf_enc_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHbbQqosfToY"
      },
      "source": [
        "## Byte Pair Encoding\n",
        "\n",
        "What BPE does is similar to huffman coding. You iterate over the text and find out which byte pairs are occurring most frequently. Then you merge those byte pairs. Here, by byte pairs, we mean the two consecutive pairs of bytes.\n",
        "\n",
        "This function is usually called `get_stats` and that's what we are also calling it here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_icfDwtePNQ",
        "outputId": "b84aa085-7a72-4134-cb1a-b63248aed660"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(101, 32)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_stats(ids):\n",
        "    counts = Counter(zip(ids, ids[1:]))\n",
        "    return counts\n",
        "\n",
        "counts = get_stats(tokens)\n",
        "top_pair = counts.most_common(1)[0][0]\n",
        "top_pair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxD67qoa2Ncs"
      },
      "source": [
        "The way to interpret this would be to say that the most common pair of bytes in this sequence has ids (101) followed by (32), and this has count equal 20. We can find the characters by using `chr` function in Python. It happens that these two characters are `e` followed by a space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6f5xQNy2u7B"
      },
      "source": [
        "Now we can write a merge function that replaces every pair of `(101, 32)` with some new character. Notice that even though some characters have multiple bytes, when we are thinking in terms of integer tokens for those, we still have a single byte for each number. None of the numbers in our list are > 255. Some fancy characters like emojis may have multiple numbers, one after the other, but they are still within the range `[0, 255]`. Thus, if we want to create a new token that indicates a merged character `(101, 32)`, then we have to assign this merged character the number 256. The number 256 represents (101, 32)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v1fC1Qm_2uRi"
      },
      "outputs": [],
      "source": [
        "def merge(ids, pair, new_idx):\n",
        "    new_ids = [ ]\n",
        "    i = 0\n",
        "\n",
        "    while i < len(ids):\n",
        "        # replace all instances of the pair with the new_idx\n",
        "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
        "            new_ids.append(new_idx)\n",
        "            i += 2\n",
        "        # append all other tokens as is\n",
        "        else:\n",
        "            new_ids.append(ids[i])\n",
        "            i += 1\n",
        "    return new_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tlzxTXGOZ0f",
        "outputId": "4464aad8-a8c7-422d-ab82-5ff54eca889e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "596"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_after_one_merge = merge(tokens, top_pair, new_idx=256)\n",
        "len(tokens_after_one_merge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzCaqXHjOBhg"
      },
      "source": [
        "As we can see, the number of tokens now is reduced since we merged a few tokens.\n",
        "\n",
        "This was one merge. If we do this iteratively, we will get more tokens and that's that!\n",
        "\n",
        "How many times should you do the merge operation? That's a hyperparameter based on hardware, etc. constraints. The more the number of tokens, the greater the storage and compute requirement. But the smaller the number of tokens, the shorter the vocabulary but the bigger the sequence length.\n",
        "\n",
        "GPT-4 uses around 100k tokens in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AcqXvSoYPQsx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merging (101, 32) into a new token 256\n",
            "Merging (240, 159) into a new token 257\n",
            "Merging (226, 128) into a new token 258\n",
            "Merging (105, 110) into a new token 259\n",
            "Merging (115, 32) into a new token 260\n",
            "Merging (97, 110) into a new token 261\n",
            "Merging (116, 104) into a new token 262\n",
            "Merging (257, 133) into a new token 263\n",
            "Merging (257, 135) into a new token 264\n",
            "Merging (97, 114) into a new token 265\n",
            "Merging (239, 189) into a new token 266\n",
            "Merging (258, 140) into a new token 267\n",
            "Merging (267, 264) into a new token 268\n",
            "Merging (101, 114) into a new token 269\n",
            "Merging (111, 114) into a new token 270\n",
            "Merging (116, 32) into a new token 271\n",
            "Merging (259, 103) into a new token 272\n",
            "Merging (115, 116) into a new token 273\n",
            "Merging (261, 100) into a new token 274\n",
            "Merging (32, 262) into a new token 275\n"
          ]
        }
      ],
      "source": [
        "desired_vocab_size = 276\n",
        "num_of_merges = desired_vocab_size - 256 # because we already have 256 tokens in our vocab\n",
        "ids = list(tokens) # make a copy of the original tokens list\n",
        "\n",
        "merges = { } #  (int, int) -> int\n",
        "for i in range(num_of_merges):\n",
        "    stats = get_stats(ids)\n",
        "    pair = max(stats, key=stats.get)\n",
        "    new_idx = 256 + i\n",
        "\n",
        "    print(f\"Merging {pair} into a new token {new_idx}\")\n",
        "    ids = merge(ids, pair, new_idx=new_idx)\n",
        "    merges[pair] = new_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZaSsvfARQoL"
      },
      "source": [
        "Notice how merged tokens can be merged even further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxCULUtwQpGF"
      },
      "source": [
        "If you think about it, you are creating a binary forest of the merges. Each time you're merging two tokens, so you have two children and a new parent token, and you're doing this from the leaves up. Not all tokens are going to get merged into a single tree, like Huffman encoding, but you're going to end up with several binary trees.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5b14wSORU6k"
      },
      "source": [
        "**Compression Ratio:**\n",
        "\n",
        "Initially, you start off with all characters in your vocabulary. Tokenizing in this way gives us a length that is equal to the string length ( or slightly more due to multi-byte encoding of some characters ). But after merging, if you tokenize again, you're going to get a token length that is less than the original text.\n",
        "\n",
        "This reduction is measured by compression ratio:\n",
        "$$\\dfrac{len(tokens)}{len(newtokens)}$$\n",
        "\n",
        "\n",
        "The more the number of merges, the greater this compression ratio would be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7Kks1eoSHJw"
      },
      "source": [
        "Tokenizer are **completely** separate stage than the large language model. Typically, this is the preprocessing stage, which may have its own data, and training stage. Once it is trained on some corpus, you can use this tokenizer to encode and decode the text which can be used with the LLM.\n",
        "\n",
        "Typically, you run tokenizer on all the raw text data that you have gathered to train your LLM on. Once you have the tokens, you can store them on disk and get rid of the text data, and work with tokens hereonafter.\n",
        "\n",
        "Other considerations are the languages to support, different encodings, etc. that you want in your language model when training this tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1_WDCbMsagQ"
      },
      "source": [
        "## Encoding and Decoding\n",
        "\n",
        "Now that we have a training algorithm that will give us the tokens, we would want to encode text into tokens and decode from the tokens.\n",
        "\n",
        "### Decoding\n",
        "\n",
        "In the decoding stage, we would want to accept a sequence of integers in the range $ [0, \\text{vocabsize}] $, and produce the corresponding text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'aï¿½b'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Construct an intermediate variable for integer to bytes mapping\n",
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "\n",
        "# Add merged pairs to the integer-bytes mapping\n",
        "for (p0, p1), idx in merges.items():\n",
        "    vocab[idx] = vocab[p0] + vocab[p1] # Concatenate bytes of the pairs\n",
        "\n",
        "def decode(ids):\n",
        "    # Get the bytes representation of the idx\n",
        "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
        "\n",
        "    # Decode the bytes into a string\n",
        "    text = tokens.decode('utf-8', 'replace')\n",
        "    return text\n",
        "\n",
        "decode([97, 128, 98])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your LLM predicts bad tokens, then you might not get valid utf-8 tokens. So that's why we need the `replace` parameter because otherwise we won't be able to decode the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding\n",
        "\n",
        "Given a text, we want to convert it into a list of integers. But remember some of the characters are now merged. So we need to merge the text as well in the same order in which we merged when we trained the tokenizer. \n",
        "\n",
        "That means we want to find the pair which has the pair which has the lowest index in the merges dictionary (because merges was `(pair): idx`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([104, 101, 108, 108, 111, 32, 119, 270, 108, 100], 'hello world')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def encode(text:str):\n",
        "    tokens = list(text.encode('utf-8'))\n",
        "\n",
        "    while len(tokens) >= 2: # number of tokens needs to be at least 2 if it needs to be considered as a pair\n",
        "        stats = get_stats(tokens)\n",
        "        pair = min(stats, key=lambda p:merges.get(p, float('inf')))\n",
        "        \n",
        "        if pair not in merges:\n",
        "            break # nothing to merge\n",
        "        \n",
        "        # If something to merge, replace old tokens\n",
        "        new_idx = merges[pair]\n",
        "        tokens = merge(tokens, pair, new_idx)\n",
        "\n",
        "    return tokens\n",
        "encode(\"hello world\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "makemore",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
